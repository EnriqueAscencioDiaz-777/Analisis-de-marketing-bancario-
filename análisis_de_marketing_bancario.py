# -*- coding: utf-8 -*-
"""Análisis de marketing bancario

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13Q58070xcmNVDzEB6ApCJLu8UFSaKW_G

#Librerias
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
import scipy.stats as stats
import sklearn as sk

"""#DataSet


*  Los datos están relacionados con campañas de marketing directo (llamadas telefónicas) de una institución bancaria portuguesa. El objetivo de la clasificación es predecir si el cliente suscribirá un depósito a plazo




"""

from google.colab import drive
    drive.mount('/content/drive')

#datosor = pd.read_csv('/content/bank-additional-full.csv', sep=';')

datosor = pd.read_csv('/content/drive/MyDrive/bank-additional-full.csv', sep=';')
datosor.head(5)

"""#Estadisticas Variables Nominales

"""

def estadisticas(df,columna):

    estadisticas = {
        "Estadísticas": [
            "Media", "Mediana", "Desviación estándar", "Varianza",
            "Mínimo", "Máximo", "Rango", "Asimetría", "Curtosis"
        ],
        "Valor": [
            df[columna].mean(),
            df[columna].median(),
            df[columna].std(),
            df[columna].var(),
            df[columna].min(),
            df[columna].max(),
            df[columna].max() - df[columna].min(),
            df[columna].skew(),
            df[columna].kurt()
        ]
    }

    return pd.DataFrame(estadisticas)

"""##Age


* Los datos presentan valores relativamente bajos y distribución moderadamente sesgada.



"""

estadisticas(datosor,'age')

plt.hist(datosor['age'])
plt.show()

"""##Last contact duration, in seconds.


*   Los datos presentan valores significativamente más altos y una distribución muy asimétrica.



"""

estadisticas(datosor,'duration')

plt.hist(datosor['duration'])
plt.show()

"""##Number of contacts performed during this campaign and for this client.


* Los datos presnetan valores relativamente bajos y una distribución fuertemente sesgada a la derecha.




"""

estadisticas(datosor,'campaign')

plt.hist(datosor['campaign'])
plt.show()

"""##Number of days that passed by after the client was last contacted from a previous campaign.


*  Los datos presentan valores altos y una distribución fuertemente sesgada a la izquierda.



"""

estadisticas(datosor,'pdays')

plt.hist(datosor['pdays'])
plt.show()

"""##Number of contacts performed before this campaign and for this client.

* Los datos presentan valores relativamente bajos y distribución moderadamente sesgada.



"""

estadisticas(datosor,'previous')

plt.hist(datosor['previous'])
plt.show()

"""##Employment variation rate


"""

estadisticas(datosor,'emp.var.rate')

plt.hist(datosor['emp.var.rate'])
plt.show()

"""##Consumer price index

"""

estadisticas(datosor,'cons.price.idx')

plt.hist(datosor['cons.price.idx'])
plt.show()

"""##Cosumer confidence index"""

estadisticas(datosor,'cons.conf.idx')

plt.hist(datosor['cons.conf.idx'])
plt.show()

"""##Euribor a 3 meses

"""

estadisticas(datosor,'euribor3m')

plt.hist(datosor['euribor3m'])
plt.show()

# prompt: DAME UN CUADRO CON LOS HISTOGRAMAS DE LAS VARIABLES NUEMRICAS

import matplotlib.pyplot as plt

# Assuming 'datosor' DataFrame is already loaded as in your provided code

numerical_cols = ['age', 'duration', 'campaign', 'pdays', 'previous', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m']
num_plots = len(numerical_cols)
num_cols = 3  # Number of columns in the subplot grid
num_rows = (num_plots + num_cols - 1) // num_cols  # Calculate the number of rows

fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows))
axes = axes.flatten()  # Flatten the axes array for easier iteration

for i, col in enumerate(numerical_cols):
    axes[i].hist(datosor[col])
    axes[i].set_title(f'Histograma of {col}')
    axes[i].set_xlabel(col)
    axes[i].set_ylabel('Frequencia')

# Turn off any unused subplots
for j in range(i + 1, len(axes)):
    axes[j].axis('off')

plt.tight_layout()
plt.show()

# prompt: DAME UNA TABLA CON LAS ESTADISTICAS DE LAS VARIBALES NUMERICAS EN UN NUNICO DATAFRAME

import pandas as pd

def estadisticas(df):
    numeric_cols = df.select_dtypes(include=['number']).columns
    stats_df = pd.DataFrame()
    for col in numeric_cols:
        stats = {
            "Variable": col,
            "Media": df[col].mean(),
            "Mediana": df[col].median(),
            "Desviación estándar": df[col].std(),
            "Varianza": df[col].var(),
            "Mínimo": df[col].min(),
            "Máximo": df[col].max(),
            "Rango": df[col].max() - df[col].min(),
            "Asimetría": df[col].skew(),
            "Curtosis": df[col].kurt()
        }
        stats_df = pd.concat([stats_df, pd.DataFrame([stats])], ignore_index=True)
    return stats_df

# Assuming 'datosor' DataFrame is already defined from the previous code
estadisticas_df = estadisticas(datosor)
estadisticas_df

"""#Clasificaion Variables"""

# prompt: Dame un codigo para clasificar las variables numericas y categoricas y damelas en una tabla

import pandas as pd
# Clasificación de variables numéricas y categóricas

# Obtener información sobre los tipos de datos de cada columna
data_types = datosor.dtypes

# Crear listas para almacenar las variables numéricas y categóricas
numerical_vars = []
categorical_vars = []

# Iterar sobre las columnas y clasificarlas
for column, dtype in data_types.items():
  if dtype in ['int64', 'float64']:
    numerical_vars.append(column)
  else:
    categorical_vars.append(column)

# Crear un DataFrame para mostrar la clasificación
classification_df = pd.DataFrame({
    'Variable': numerical_vars + categorical_vars,
    'Tipo': ['Numérica'] * len(numerical_vars) + ['Categórica'] * len(categorical_vars)
})

# Mostrar la tabla
classification_df

# prompt: Dame un codigo para clasificar las variables numericas y categoricas y damelas en diccionarios

import numpy as np
import pandas as pd

# Assuming 'datosor' DataFrame is already loaded as in your provided code

# Identify numerical and categorical features
numerical_features = datosor.select_dtypes(include=np.number).columns.tolist()
categorical_features = datosor.select_dtypes(exclude=np.number).columns.tolist()

# Create dictionaries
numerical_dict = {'numerical_features': numerical_features}
categorical_dict = {'categorical_features': categorical_features}

print(numerical_dict)
categorical_dict

"""# Limpieza de Datos

##Variables categoricas
"""

#Prorcentaje de datos nulos.
datosor.isna().mean()*100

"""
###Job"""

datosor.groupby('job').size()

datosor['job']= datosor['job'].str.replace('-','_')
datosor['job']=datosor['job'].str.replace('.','')
datosor['job']= datosor['job'].str.strip()
datosor.groupby('job').size()

frecuencias_job = datosor['job'].value_counts()
frecuencias_job.plot(kind='bar')
plt.show()

"""###Education"""

datosor.groupby('education').size()

datosor['education'] = datosor['education'].astype(str)
datosor['education'] = datosor['education'].str.strip()
datosor['education'] = datosor['education'].str.replace('.', '')
datosor['education_limpio'] = datosor['education'].apply(lambda x: 'basic' if x in ['basic4y', 'basic6y', 'basic9y'] else x)
datosor.groupby('education_limpio').size()

datosor['education_limpio'] = datosor['education_limpio'].fillna('unknown')
datosor.drop('education', axis=1, inplace=True)
frecuencias_education_limpio = datosor['education_limpio'].value_counts()
frecuencias_education_limpio.plot(kind='bar')
plt.show()

datosor.rename(columns={'education_limpio': 'education'}, inplace=True)

"""###Marital

"""

datosor.groupby('marital').size()

frecuencias_marital = datosor['marital'].value_counts()
frecuencias_marital.plot(kind='bar')
plt.show()

"""###Default"""

datosor.groupby('default').size()

frecuencias_default= datosor['default'].value_counts()
frecuencias_default.plot(kind='bar')
plt.show()

"""###Housing"""

datosor.groupby('housing').size()

frecuencias_housting = datosor['housing'].value_counts()
frecuencias_housting.plot(kind='bar')
plt.show()

"""###Loan"""

datosor.groupby('loan').size()

frecuencias_loan = datosor['loan'].value_counts()
frecuencias_loan.plot(kind='bar')
plt.show()

"""###Contact  


"""

datosor.groupby('contact').size()

frecuencias_contact = datosor['contact'].value_counts()
frecuencias_contact.plot(kind='bar')
plt.show()

"""###Month"""

datosor.groupby('month').size()

frecuencias_month = datosor['month'].value_counts()
frecuencias_month.plot(kind='bar')
plt.show()

"""###Day of week"""

datosor.groupby('day_of_week').size()

frecuencias_day_of_week = datosor['day_of_week'].value_counts()
frecuencias_day_of_week.plot(kind='bar')
plt.show()

"""###Poutcome"""

datosor.groupby('poutcome').size()

frecuencias_poutcome = datosor['poutcome'].value_counts()
frecuencias_poutcome.plot(kind='bar')
plt.show()

"""###Y (contrato servicio)"""

datosor.groupby('y').size()

frecuencias_y = datosor['y'].value_counts()
frecuencias_y.plot(kind='bar')
plt.show()

"""###Contacted"""

plt.hist(datosor['pdays'])
plt.xlabel('Pdays_sesgada')
plt.ylabel('Frecuencia')
plt.show()

# Function to categorize 'pdays'
def categorizar_pdays(valor):
    if valor == 999:
        return 'no'
    else:
        return 'yes'

# Apply the function and create the new column in 'datosor'
datosor['contacted'] = datosor['pdays'].apply(categorizar_pdays)

datosor.groupby('contacted').size()

datosor.drop('pdays', axis=1, inplace=True)

datosor.head()

frecuencias_contacted = datosor['contacted'].value_counts()
frecuencias_contacted.plot(kind='bar')
plt.show()

"""#Z-score_RIC

"""

for columnas in datosor.columns:
 print(columnas)

def z_score(df, columna):
  df[columna+ 'z'] = (df[columna] - df[columna].mean())/df[columna].std()
  df_filtrado = df[df[columna + 'z'].abs()<2]
  return df_filtrado

def outliers_RIC(df, columna):
    Q1 = df[columna].quantile(0.25)
    Q3 = df[columna].quantile(0.75)
    RIC = Q3 - Q1
    cota_inferior = Q1 - 1.5 * RIC
    cota_superior = Q3 + 1.5 * RIC
    df_filtrado = df[(df[columna] >= cota_inferior) & (df[columna] <= cota_superior)]
    return df_filtrado

"""###Age"""

datos_filtrados_age = datosor[(datosor['age']>0) & (datosor['age']<100) == True]

plt.hist(datos_filtrados_age['age'])
plt.xlabel('Age')
plt.show()

plt.boxplot(datos_filtrados_age['age'])
plt.show()

datosor = z_score(datos_filtrados_age,'age')
plt.boxplot(datosor['age'])
plt.show()

"""###Duration"""

plt.hist(datosor['duration'])
plt.xlabel('Duracion')
plt.show()

plt.boxplot(datosor['duration'])
plt.show()

datosor = z_score(datosor,'duration')
plt.boxplot(datosor['duration'])
plt.show()

"""###Campaing

"""

plt.hist(datosor['campaign'])
plt.xlabel('Campaing')
plt.show()

plt.boxplot(datosor['campaign'])
plt.show()

datosor = outliers_RIC(datosor,'campaign')
plt.boxplot(datosor['campaign'])
plt.show()

"""###previous"""

plt.hist(datosor['previous'])
plt.xlabel('previous')
plt.show()

plt.boxplot(datosor['previous'])
plt.show()

datosor = outliers_RIC(datosor,'previous')
plt.boxplot(datosor['previous'])
plt.show()

"""###emp.var.rate

"""

plt.hist(datosor['emp.var.rate'])
plt.xlabel('emp.var.rate')
plt.show()

plt.boxplot(datosor['emp.var.rate'])
plt.show()

datosor = outliers_RIC(datosor,'emp.var.rate')
plt.boxplot(datosor['emp.var.rate'])
plt.show()

"""###cons.price.idx"""

plt.hist(datosor['cons.price.idx'])
plt.xlabel('cons.price.idx')
plt.show()

plt.boxplot(datosor['cons.price.idx'])
plt.show()

datosor = outliers_RIC(datosor,'cons.price.idx')
plt.boxplot(datosor['cons.price.idx'])
plt.show()

"""###cons.conf.idx"""

plt.hist(datosor['cons.conf.idx'])
plt.xlabel('cons.conf.idx')
plt.show()

plt.boxplot(datosor['cons.conf.idx'])
plt.show()

datosor = outliers_RIC(datosor,'cons.conf.idx')
plt.boxplot(datosor['cons.conf.idx'])
plt.show()

"""###euribor3m"""

plt.hist(datosor['euribor3m'])
plt.xlabel('euribor3m')
plt.show()

plt.boxplot(datosor['euribor3m'])
plt.show()

datosor = outliers_RIC(datosor,'euribor3m')
plt.boxplot(datosor['euribor3m'])
plt.show()

"""###nr.employed"""

plt.hist(datosor['nr.employed'])
plt.xlabel('nr.employed')
plt.show()

plt.boxplot(datosor['nr.employed'])
plt.show()

datosor = outliers_RIC(datosor,'nr.employed')
plt.boxplot(datosor['nr.employed'])
plt.show()

"""#Valores Redundates

"""

datosor.drop('previous', axis=1, inplace=True) ##Tras la limpieza obtubimos valores nulos

matriz_covarianza = datosor[['age','duration','campaign','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed']].cov()
matriz_covarianza

sns.heatmap(datosor[['age','duration','campaign','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed']].corr(), annot=True)
plt.show()

sns.pairplot(datosor[['age','duration','campaign','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed']])
plt.show()

#Correlacíon de Kendal
correlacion_kendall = datosor[['age','duration','campaign','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed']].corr(method='kendall')
correlacion_kendall

sns.heatmap(correlacion_kendall, annot=True)
plt.show()

#Correlacion de spearman
correlacion_spearman = datosor[['age','duration','campaign','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed']].corr(method='spearman')
correlacion_spearman

sns.heatmap(correlacion_spearman, annot=True)
plt.show()

"""#Regresion lineal

"""

from sklearn.linear_model import LinearRegression
import pandas as pd

variables = []
coeficientes = []

variables_continuas = ['age', 'duration', 'campaign', 'emp.var.rate',
                       'cons.price.idx', 'cons.conf.idx']

# Create a DataFrame with the continuous variables
df_continuas = datosor[variables_continuas]

for variable in variables_continuas:
    y = df_continuas[variable]  # Access the column using the variable name
    X = df_continuas.drop(variable, axis=1)  # Drop the target variable from features
    modelo = LinearRegression()
    modelo.fit(X, y)

    # Assuming 'x' should be 'X' for prediction:
    coeficientes.append(modelo.score(X, y))
    variables.append(variable)

resumen_de_datos = pd.DataFrame({'variable': variables, 'coeficientes': coeficientes})
resumen_de_datos

datosor.head()

#Iteracion 1
datosor = datosor.drop(['nr.employed','euribor3m','emp.var.rate'], axis=1)

"""##Informacion Mutua"""

from sklearn.feature_selection import mutual_info_classif
datos_categoricos = datosor[['job',
  'marital',
  'default',
  'housing',
  'loan',
  'contact',
  'month',
  'day_of_week',
  'y',
  'education']]

datosor.head()

import pandas as pd
from sklearn.metrics import mutual_info_score

df_cuadratico = pd.DataFrame(index=datos_categoricos.columns, columns=datos_categoricos.columns)

import pandas as pd
from sklearn.metrics import mutual_info_score # Make sure to import the function in the current cell

df_cuadratico = pd.DataFrame(index=datos_categoricos.columns, columns=datos_categoricos.columns)

for x in datos_categoricos.columns:
  for y in datos_categoricos.columns:
    informacion_mutua= mutual_info_score(datos_categoricos[x], datos_categoricos[y])
    df_cuadratico.loc[x,y] = informacion_mutua
df_cuadratico

"""##Redundancia de Theil"""

# Implementación (simplificada)
def redundancia_theil(x, y):
  from collections import Counter

  # Calcula las probabilidades conjuntas y marginales
  c = Counter(zip(x, y))
  n = len(x)
  p_xy = {(xx, yy): count / n for (xx, yy), count in c.items()}
  p_x = Counter(x)
  p_y = Counter(y)

  # Calcula la redundancia de Theil
  theil_u = 0
  for (xx, yy), p in p_xy.items():
    theil_u += p * np.log(p / (p_x[xx] / n * p_y[yy] / n))

  return theil_u / (np.sqrt(sum([(p_x[xx] / n) * np.log(p_x[xx] / n) ** 2 for xx in p_x])) *
                    np.sqrt(sum([(p_y[yy] / n) * np.log(p_y[yy] / n) ** 2 for yy in p_y])))

"""##Coeficiente de Cramer"""

# prompt: Genera el codigo para el coeficente de cramer

import pandas as pd
import numpy as np
def cramers_v(x, y):
    confusion_matrix = pd.crosstab(x, y)
    chi2 = stats.chi2_contingency(confusion_matrix)[0]
    n = confusion_matrix.sum().sum()
    phi2 = chi2 / n
    r, k = confusion_matrix.shape
    # Handle the case where r or k is 1
    if r == 1 or k == 1:
        return 0
    phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))
    rcorr = r - ((r - 1) ** 2) / (n - 1)
    kcorr = k - ((k - 1) ** 2) / (n - 1)
    # Handle the case where rcorr or kcorr is 0 or negative
    if rcorr <= 0 or kcorr <= 0:
        return 0
    return np.sqrt(phi2corr / min((kcorr - 1), (rcorr - 1)))

# Calculate Cramer's V for all pairs of categorical features
cramer_matrix = pd.DataFrame(index=datos_categoricos.columns, columns=datos_categoricos.columns)

for col1 in datos_categoricos.columns:
    for col2 in datos_categoricos.columns:
        cramer_matrix.loc[col1, col2] = cramers_v(datos_categoricos[col1], datos_categoricos[col2])

cramer_matrix

from scipy.stats import chi2_contingency

# Crea una tabla de contingencia entre dos variables categóricas
#tabla_contingencia = pd.crosstab(datos_categoricos['variable1'], datos_categoricos['variable2'])

# Calcula el coeficiente de Cramer
#chi2, p, dof, expected = chi2_contingency(tabla_contingencia)
#n = tabla_contingencia.sum().sum()
#phi2 = chi2 / n
#v_cramer = np.sqrt(phi2 / min(tabla_contingencia.shape[0] - 1, tabla_contingencia.shape[1] - 1))

# Imprime el coeficiente de Cramer
#print(v_cramer)

"""#Valores atípicos

##Distancia de Mahalanobis
"""

# Importamos la libreria que ocuparemos
from scipy.spatial import distance

import numpy as np
datos_numericos = datosor.select_dtypes(include=np.number)

# Calculamos la matriz de covarianza inversa
cov_inv = np.linalg.inv(datos_numericos.cov())

# Calculamos la distancia de Mahalanobis para cada punto de datos
centroide = datos_numericos.mean()
distancias_mahalanobis = datos_numericos.apply(lambda fila: distance.mahalanobis(fila,
                                                                                 centroide,
                                                                                 cov_inv), axis=1)

# Añadimos la distancia de Mahalanobis como una nueva columna al Dataframe
datosor['distancia_mahalanobis'] = distancias_mahalanobis

# Identificamos los valores atipicos
umbral = np.percentile(distancias_mahalanobis, 95)  # Umbral del percentil 95
valores_atipicos = datosor[datosor['distancia_mahalanobis'] > umbral]
valores_atipicos

datos_sin_atipicos = datosor[datosor['distancia_mahalanobis'] <= umbral]

# Se restablece el índice
datosor = datos_sin_atipicos.reset_index(drop=True)

datosor

"""#Prueba Chi-Cuadrada

###Prueba de independecia
"""

from scipy.stats import chi2_contingency

# Selecciona las dos variables categóricas
variable1 = 'education'
variable2 = 'job'

# Crea una tabla de contingencia
tabla_contingencia = pd.crosstab(datosor[variable1], datosor[variable2])

# Realiza la prueba de independencia
chi2, p, dof, expected = chi2_contingency(tabla_contingencia)

# Imprime los resultados
print(f"Estadística de chi-cuadrada: {chi2}")
print(f"Valor p: {p}")

# Si p < 0.05, rechaza la hipótesis nula de independencia
alpha = 0.05

if p < alpha:
    print("Existe una asociación significativa entre las variables.")
else:
    print("No existe una asociación significativa entre las variables.")

"""###Prueba de Bondad y Ajuste

"""

import pandas as pd
from scipy.stats import chisquare

# 1. Seleccionar la variable categórica
variable = "education"

# 2. Definir la distribución esperada (uniforme en este caso)
# Se asume que cada categoría tiene la misma probabilidad

# 3. Calcular las frecuencias observadas y esperadas
observadas = pd.DataFrame(datosor[variable].value_counts())
num_categorias = len(observadas)
total_observaciones = observadas['count'].sum()
esperadas = pd.DataFrame([total_observaciones / num_categorias] * num_categorias,
                         index=observadas.index, columns=[variable])

# 4. Realizar la prueba de chi-cuadrada
chi2_statistic, p_value = chisquare(observadas['count'], f_exp=esperadas[variable])

# Imprimir los resultados
print(f"Variable: {variable}")
print(f"Frecuencias observadas:\n{observadas}\n")
print(f"Frecuencias esperadas:\n{esperadas}\n")
print(f"Estadística de chi-cuadrada: {chi2_statistic}")
print(f"Valor p: {p_value}")

# Interpretar los resultados
alpha = 0.05  # Nivel de significancia
if p_value < alpha:
    print(f"Se rechaza la hipótesis nula. La distribución de {variable} no se ajusta a una distribución uniforme.")
else:
    print(f"No se rechaza la hipótesis nula. No hay suficiente evidencia para sugerir que la distribución de {variable} no se ajusta a una distribución uniforme.")

"""#Caras de Chernof

"""

!pip install carat
import carat
print(carat.__version__)

# prompt: define la variable de datos_categoricos del set de datos datosor

datos_categoricos = datosor[['job', 'marital', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome', 'y', 'education']]

# Importando chernoff_face desde carat.
from carat import *
# Seleccionando una muestra pequeña de los datos
muestra_datos = datos_categoricos.sample(20)
muestra_datos

"""# PSA"""

# prompt: Realiza el codigo para el analisis por PSA

import pandas as pd
import matplotlib.pyplot as plt
# Convert categorical variables to numerical using one-hot encoding
datos_encoded = pd.get_dummies(datosor, columns=categorical_features, drop_first=True)

# Separate features (X) and target (y)
X = datos_encoded.drop('y_yes', axis=1)  # Assuming 'y_yes' is the target variable after one-hot encoding
y = datos_encoded['y_yes']

# Calculate PSA (Principal Sensitivity Analysis)
# PSA is not a standard single function like PCA. It's a methodology.
# A common approach is to use logistic regression and analyze coefficients or feature importance.
# Let's use Logistic Regression coefficients as a simple indicator of sensitivity.

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

# Scale numerical features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train a Logistic Regression model
model = LogisticRegression(solver='liblinear') # Use liblinear for smaller datasets
model.fit(X_scaled, y)

# Get the coefficients
coefficients = model.coef_[0]

# Create a DataFrame of features and their coefficients
psa_results = pd.DataFrame({'feature': X.columns, 'coefficient': coefficients})

# Sort features by the absolute value of their coefficients to see the most influential ones
psa_results['abs_coefficient'] = abs(psa_results['coefficient'])
psa_results = psa_results.sort_values(by='abs_coefficient', ascending=False)

print("Principal Sensitivity Analysis (using Logistic Regression Coefficients):")
print(psa_results)

# Visualize the coefficients
plt.figure(figsize=(10, 8))
sns.barplot(x='abs_coefficient', y='feature', data=psa_results.head(20)) # Display top 20
plt.title('Top 20 Features by Absolute Logistic Regression Coefficient (PSA)')
plt.xlabel('Absolute Coefficient Value')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()

# Another perspective of PSA could involve analyzing the model's performance
# with subsets of features or by varying the sensitivity of the model's decision boundary.
# This is a more complex analysis and depends on the specific goal of sensitivity analysis.
# For a simple illustration, we've used the model coefficients as a proxy for feature sensitivity.

"""#EMS

#CLUSTERING
"""

# prompt: Realiza el clustering con los daros limpios

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Assuming 'datosor' is your cleaned DataFrame from the previous steps

# Select the numerical columns for clustering
numerical_cols_for_clustering = datosor.select_dtypes(include=np.number).columns.tolist()

# Exclude any columns that were created during the analysis (like 'distancia_mahalanobis' or z-score columns)
# Adapt this list based on which columns you want to use for clustering
cols_to_exclude = ['distancia_mahalanobis']
numerical_cols_for_clustering = [col for col in numerical_cols_for_clustering if col not in cols_to_exclude and not col.endswith('_z')]

X_clustering = datosor[numerical_cols_for_clustering]

# Scale the numerical data
scaler_clustering = StandardScaler()
X_scaled_clustering = scaler_clustering.fit_transform(X_clustering)

# Determine the optimal number of clusters using the Elbow Method
inertia = []
k_range = range(1, 11)  # Test K from 1 to 10

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10) # Set n_init to explicitly avoid warning
    kmeans.fit(X_scaled_clustering)
    inertia.append(kmeans.inertia_)

# Plot the Elbow Method graph
plt.figure(figsize=(8, 5))
plt.plot(k_range, inertia, marker='o')
plt.title('Método del Codo para determinar el número óptimo de clusters')
plt.xlabel('Número de Clusters (K)')
plt.ylabel('Inercia')
plt.xticks(k_range)
plt.grid(True)
plt.show()

# Based on the Elbow Method, choose a suitable number of clusters (e.g., if the elbow is at K=3 or 4)
# Let's assume the elbow is at K=3 for demonstration purposes. Adjust based on your plot.
optimal_k = 3

# Perform K-Means clustering with the chosen number of clusters
kmeans_optimal = KMeans(n_clusters=optimal_k, random_state=42, n_init=10) # Set n_init to explicitly avoid warning
kmeans_optimal.fit(X_scaled_clustering)

# Add the cluster labels to the original DataFrame
datosor['cluster'] = kmeans_optimal.labels_

# Analyze the characteristics of each cluster
print(datosor.groupby('cluster')[numerical_cols_for_clustering].mean())

# You can also analyze the distribution of categorical variables within each cluster
# For example:
for cat_col in categorical_features:
    print(f"\nDistribución de '{cat_col}' por Cluster:")
    print(pd.crosstab(datosor['cluster'], datosor[cat_col]))

# Visualize the clusters (e.g., using PCA or selecting two principal components)
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled_clustering)

pca_df = pd.DataFrame(data=X_pca, columns=['PCA1', 'PCA2'])
pca_df['cluster'] = datosor['cluster'] # Add cluster labels

plt.figure(figsize=(10, 7))
sns.scatterplot(x='PCA1', y='PCA2', hue='cluster', data=pca_df, palette='viridis', legend='full')
plt.title('Clusters visualizados en los primeros dos componentes principales (PCA)')
plt.xlabel('Componente Principal 1')
plt.ylabel('Componente Principal 2')
plt.show()

# You can also try visualizing with other pairs of features or dimensionality reduction techniques like t-SNE (though t-SNE is computationally more expensive).

